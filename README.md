# csts_os_code
Project Overview

This repository provides the implementation for the experiments presented in our paper. It includes code for interacting with LLMs, data preparation, model training, evaluation, and explanation selection.
Files and Directories
LLM_request_sample.py

    Demonstrates how to query models such as DeepSeekV3 to obtain one or multiple candidate answers.

    Also shows the format for querying lightweight models such as LLaMA3.2-1B for single or multiple candidate answers.

exp1/ — For Experiment 4.2

    Implements the pipeline for Section 4.2.

    Run exp1_data_prepare.py to generate the training and test datasets with different configurations.

    Then, run exp1_train_eval.py to train the model and evaluate its performance.

exp2/ — For Experiment 4.3

    Implements the full workflow described in Section 4.3.

    Start by executing exp2_data_prepare.py to generate the training and test datasets.

    Use exp2_model_train.py for model training.

    Use exp2_eval.py for evaluation.

    exp2_tok_model.py is for training models with different sentence encoding strategies.

    exp2_tok_eval.py is for evaluating the performance of different sentence encodings.

    Note: When using reserved tokens in LLaMA-series models as sentence encoders, modify the tokenizer.json in the LLaMA model directory by replacing "<|reserved_special_token_0|>" with "<|end|>".

exp3/ — Explanation Selection and Final Evaluation

    Selects the best explanation from multiple candidate answers generated by different models.

    Computes a variety of performance metrics based on the selected optimal explanation.
